%\VignetteIndexEntry{faoswsFlag:A package to perform flag aggregation and much more}
%\VignetteEngine{knitr::knitr}
\documentclass[nojss]{jss}
\usepackage{url}
\usepackage[sc]{mathpazo}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{breakurl}
\usepackage{hyperref}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{mathtools}
%% \usepackage{draftwatermark}
\usepackage{float}
\usepackage{placeins}
\usepackage{mathrsfs}
\usepackage{multirow}
%% \usepackage{mathbbm}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator*{\argmax}{\arg\!\max}
%% \linespread{1.3}
\setlength{\parskip}{1em}

\title{\bf Parameterisation of Food Balance Sheet Uncertainty Distribution}

\author{Michael. C. J. Kao\\ Food and Agriculture Organization \\ of
  the United Nations}

\Plainauthor{Michael. C. J. Kao} 

\Plaintitle{Parameterisation of Food Balance Sheet Uncertainty Distribution}

\Shorttitle{Parameterisation of Food Balance Sheet Uncertainty Distribution}

\Abstract{ 

  In solving the imbalance problem of the Food Balance Sheet (FBS), a
  novel solution based on probability maximisation was adopted to
  avoid the dispute of which element should be chosen to balance.
  
  The implementation of the probabilistic framework requires
  consistent and logical specification of corresponding probability
  distribution for each of the corresponding in the Food Balance
  Sheet.
  
  In this paper, we present the rationale and theory behind the
  framework. At the same time we also provide example based
  illustration to demonstrate the use of the \pkg{faoswsFlag}
  packages.
  
  
  
}

\Keywords{Uncertainty, Distribution}
\Plainkeywords{Uncertainty, Distribution}

\Address{
  Michael. C. J. Kao\\
  Economics and Social Statistics Division (ESS)\\
  Economic and Social Development Department (ES)\\
  Food and Agriculture Organization of the United Nations (FAO)\\
  Viale delle Terme di Caracalla 00153 Rome, Italy\\
  E-mail: \email{michael.kao@fao.org}\\
  URL: \url{https://github.com/mkao006/sws_r_api/tree/master/faoswsFlag}
}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(xtable)
library(RColorBrewer)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.show='hold',
               warning=FALSE, message=FALSE, error=FALSE, tidy=FALSE, 
               results='markup', eval=TRUE, echo=TRUE, cache=FALSE)
options(replace.assign=TRUE,width=80, scipen = 8)
assign("depthtrigger", 10, data.table:::.global)
@ 


\section{Introduction}

In preparing the Food Balance Sheet (FBS), one of the most
indispenable yet challenging operation is the balancing mechanism. Due
to the imperfection of data collection, estimation and imputation in
the real world, it is the norm that the FBS is unbalanced and does not
satisfy the equality constraint at first sight. Thus, a balancing
mechanism is essential for satisfying the equality between the demand
and supply of the FBS.

%% Give a footnote to the construction of the FBS.

%% Taken from the overview paper
In current practice, when imbalance exist, a variable is assigned the
balancing item and the value of the balancing item is adjusted such
that the equality constraint is satisfied. The choice of a variable as
a balancing item often reflects the availability of data (or the lack
of data), rather than a logical justification and empirical
evidence. It is therefore not surprising that different SUA
compilers/SUA approaches have chosen different variables as their
balancing items. USDA's balances, for instance, use feed (and residual
use) as the balancing item, while the FBS often use food to balance
supply and demand. Conveniently, the XCBS approach often chooses
whatever variable is not explicitly available. Clearly, none of these
approaches renders a satisfactory solution to the problem and, no
matter what variable is used as the balancing item, this variable is
fraught with the measurement errors of all other variables. Given the
fact that there is no a priori reason to assume that the measurement
errors cancel out, the balancing item is bound to be the most
inaccurate variable of the balance.  Extending the logic to the Food
Balance Sheets, using food as the balancing item would therefore be
the least suitable solution.

%% Foot note to the problems.
Problems associated with the current approach motivated the research
team to seek a method in which inequality occuring in the FBS to be
allocated to various elements based on a sound and logical reasoning
rather than arbitrary allocation. One method to handle the problem is
to balance the FBS based on a probabilistic basis. Each element is
assumed to have a pre-determined level of uncertainty, and the
allocation of the imbalance will depends on the uncertainty of each
element. That is, the smaller the uncertainty we have with a
particular element, the less we should apportion the imbalance or
adjust that particular element. In the extreme case where we have
perfect certainty about an element, than no imbalance should be added
to the element nor adjusted.


In order to proceed with the probabilistic balancing mechanism,
formulation of distribution for each of the elements in the FBS is
necessary. The specification of the distributions undermines the
validity of the balancing mechanism, and thus a consistent and logical
construction of the distribution is crucial. The distributions should
reflect the underlying uncertainty associated with each elements while
respecting the relationship amongst all elements. It is under these
conditions, the optimal solution from the probabilistic framework is
valid and justified. A framework guiding the specification of the
distributions and the parameterisations is thus required and is the
focus of this paper. The absence of such framework generates
inconsistency and paradox, further the use of the term probability
maximisation is a disguise for a procedure which does not bear any
proper interpretation.


The paper is organised as follow. A simplified dataset is presented to
familiarise the user with the problem. The subsequent section is then
devoted to the rationale and theory behind the method for constructing
the uncertainty distribution; a simple example with snippets will
accompany the theory section to elaborate on the methodology. Finally,
an application of the method for the balancing mechanism is presented
and end with discussion.

%% A corresponding table representing the weights or confindence of
%% various data sources is assumed to have been constructed. For more
%% details on the construction of the flag table, please refer to the
%% faoswsFlag vignette.



\section{The Data}

Before we delve into the theory and application, we will demonstrate
some cases of the data for back ground. Further, these data will be
later used to demonstrate the method and the package.


First of all, we can load up the package by prompting the following command.

<<presetup>>=
## Load the library and the two example datasets
library(faoswsFlag)
data(vignetteFlagTable)
data(vignetteFBSTable)
@ 


The example flag table \textit{vignetteFlagTable} is shown below.

<<flagTable, results='asis', echo=FALSE>>=
print(xtable(vignetteFlagTable, align = "ccc"), include.rownames = FALSE)
@ 

The column \textbf{flagObservationStatus} represents the flags
associated with each observed value, while the
\textbf{flagObservationWeight} represents the corresponding confidence
associated with the source. Empty flag denotes official data, while
"I" stands for imputed value and "E" are manual estimates. The weights
like probability, should be between 0 and 1.


Shown below is the \textit{vignetteFBSTable} dataset which is a
simplified version of the Food Balance Sheet for illustration in this
paper. Each observed value is associated with a flag which indicate
the source of the data. Under the proposing framework, the flag
contains information about the uncertainty of the value and will be
used to parameterise the distribution. There are more elements to the
Food Balance Sheet, however, we have selected a handful of variables
for illustrative purpose.


<<fbsTable, results='asis', echo=FALSE>>=
printTable = vignetteFBSTable[, -1]
colnames(printTable) = gsub("Value", "", colnames(printTable))
flagCol = grepl("Flag", colnames(printTable))
colnames(printTable)[flagCol] = "flag"
print(xtable(printTable, digits = 0, align = c("ccccccccccc")), include.rownames = FALSE)
@ 

In the example data, all trade data both import and export are
official. On the other hand losses and production are imputed by
statistical methods, while food was estimated based on manual
estimates.


\section{The Methodology}

In this section, we will describe the rationale and provide some
background theory followed by an example at the end.

From the example, we can observe that for each element and item in the
Food Balance Sheet (FBS), only a single value is observed. Since only
a single observation is available, the use of Frequentism method can
not be applied here. Rather, we have adopted the subjectivism
interpretation of probability in order to come up with a solution.

%% Take for example, it is common for people to make the following
%% statement, "I think the probability it will rain today is about
%% 50\%". The individual may not have collected any data, nor understand
%% the hydrological cycle, but the statement itself is valid. This is the
%% subjectivism interpretation of the probability where we

To construct a probability distribution about the value, one
first requires a chosen particular distribution, then parameterise the
distribution according to a set of standards and rules.

The choice of the distribution should reflect knowledge and known
constaints about the variable. The support, shape and properties of
the distribution should be guided by the expertise of the officer. For
example, production is strictly positive and thus distributions such
as the Normal or the Cauchy should be eliminated from the
set. Further, if extreme value are more likely then a Weibull
distribution may be more preferable in comparison to the truncated
normal distribution which has a higher kurtosis. 

After the distribution has chosen, then the distribution need to be
parameterised in order to complete the construction of the
distribution. Logically, the parameter of the distribution should
relate to the observed value and the specified confidence. Moreover,
the mode of the constructed distribution should be made to be
equivalent to the observed value. This task is in general simple, yet
the conversion of the confidence level to the dispersion parameter of
the distribution requires several more steps. 

Here we propose a method in which the confidence can be converted to a
measure of uncertainty we have about a particular value and ultimately
lead to the parameterisation of distributions.


\subsection{Quantifying Uncertainty}

To measure a piece of information, one can use the formula of
\textit{self information} which is a measure of the information
content and is defined as,

\begin{align}
  \label{eqn:si}
  I = -ln(P) \qquad P \in [0, 1]
\end{align}


Where P is the probability or the confidence about the accuracy of the
value assigned to the observed value in the first place. The natural
logarithm is adopted here, but logarithm of any base can be used. This
is a measure of the uncertainty conditioned on the confidence we have
provided about the observed value. The greater the I, the larger the
associated uncertainty, that is, the lower the confidence we assign to
the particular value, the higher the uncertainty. When the confidence
is 0, or with 0\% certainty, then I is infinite or infinite
uncertainty; on the other hand, when the confidence is 1 then the the
value is known with certain.

The logarithm also ensures that the uncertainty is
additive. Essentially, the sum of the uncertainty is the log of the
products of the probabilities assigned to the values. That is, it is
the log of the joint probability assuming independence.

The function enable us to convert the confidence about a single value
to how much uncertainty is associated with the value.


%% Research more properties of self information and entropy


\subsection{Parameterise Distribution Given Uncertainty}


Provided that we observe a single value, and at the same time our
quantifying the uncertainty about a particular value; any chosen
distribution can be parameterised accordingly to reflect the empirical
evidence and knowledge about the value.

By setting the observed value to the expected value of the
distribution (the expected value here refers to the value with the
highest probability, that is, the mode) and the self information to
the expected information or the differential entropy of the
distribution, the parameters of the chosen distribution can then be
obtained by solving the set of equations. Given the level of
uncertainty associated with each element, then regardless of the
choice of distributions, one can always parametrize the distribution
where the uncertainty is held the same. This provides a consistent
framework for specifying distributions in which the uncertainty for
each element is consistent and relative amongst all elements.

That is, we parameterise the distribution given the following identities.

\begin{align}
  Mode(X) &= x\\ \nonumber
  H(X) &= I
\end{align}

Where $X$ is the random variable and $x$ is the observed value, $I$ is
the self-information or uncertainty computed according to formula
\ref{eqn:si}, and $H$ is the differential entropy of the chosen
distribution.

The main reason to use the entropy rather than other dispersion
parameters such as the aboslute size of the standard deviation is
because it is unit free and does not depends on the size of the
value. If we were to impose uncertainty between two values, then the
uncertainty associated with both value should be set respectively to
the confidence given independent of the magnitude of the value. For
example, if we we have observed 2000 tonnes of wheat production and
1000 tonnes of food while the confidence in the two value are
identical, then the balance should be 1500 tonnes of production and
food. If we were to base the uncertainty on standard deviation or
percentage of variation, then the larger value will have large
standard deviation of variation based on percentage and thus the final
value will be closer to 1000 even though we have equal confidence in
both values.

Furthermore, both Normal and the truncated Normal distribution has a
standard deviation parameter, however, setting the two distribution
with identical standard deviation actually gives the normal
distribution a high level of uncertainty.

\subsection{A Simple Example}

The following illustration provides an example of the method, along
with codes to demonstrate the use of the package. In addition, we will
demonstrate how this framework can provide consistent parameterization
of various distribution while maintaining the same level of
uncertainty with the value. 


<<example-parameter>>=
obsValue = 20
confidence = 0.02
@ 

<<secret-conversion, echo=FALSE>>=
## confidence = confidence/10
@ 

Let us assume that we have an estimated value of \Sexpr{obsValue}
thousand tonnes of wheat production in Australia in 2010.


Then following the flag table, we have a confidence of
\Sexpr{confidence * 100}\% in the observed value. The amount of
uncertainty regarding the wheat production in Australia given the
confidence can then be calculated as,

\begin{align}
  I = -ln(\Sexpr{confidence}) \approx \Sexpr{-log(confidence)} \nonumber
\end{align}

<<calculate-self-info>>=
(selfInfo = selfInformation(confidence))
@ 

In order to preserve this uncertainty associated with this piece of
information, we need to preserve the entropy of the
distributions. That is, regardless which distribution we choose we
need to parameterise the distribution such that the entropy is
equivalent to the same nat of information available.

Now for naive reasons that we want to impose a Normal distribution on
the wheat production in Australia, we can first set the observed value
to the mode of the distribution to first give us the first parameter
of the Normal distribution.

\begin{align}
  \mu = \Sexpr{obsValue}
\end{align}


In order to solve for the standard deivation $\sigma$ of the Normal
distribution, we first re-arrange the entropy function of the Normal
distribution where the parameter $\sigma$ is a function of the entropy
$H$. Then by substituting the expected information $H$ with the
self-information of the observed value $I$, we can then obtain the
standard deviation of the distribution

Starting with the differential entropy of the Normal distribution
\begin{align}
  H &= \frac{1}{2}ln(2\pi e\sigma^2) \nonumber\\
  \intertext{and re-arrange the equation,}
  \sigma &= \sqrt{\frac{e^{2H}}{2\pi e}} \nonumber\\
  \intertext{substituting $H$ with $I$ we obtain the value of the standard deviation as}
  \sigma &= \sqrt{\frac{e^{2(-ln(\Sexpr{confidence}))}}{2\pi e}} \approx \Sexpr{round(parameterise(obsValue = obsValue, 
             selfInformation = selfInfo, 
             distribution = "normal")$sd, 4)} \nonumber
\end{align}

or simply,

<<parameterise-normal>>=
parameterise(obsValue = obsValue, 
             selfInformation = selfInfo, 
             distribution = "normal")
@ 

That is, when the observed value of wheat production in Australia is
\Sexpr{obsValue} and a confidence of \Sexpr{confidence * 100}\% is
imposed, then the associated uncertainty distribution is then:

\begin{align}
  W \sim N(\Sexpr{obsValue}, \Sexpr{round(parameterise(obsValue = obsValue, 
    selfInformation = selfInfo, 
    distribution = "normal")$sd, 4)})\nonumber
\end{align}

However, if we believe that the production is in general rather stable
over time but are subject to events such as drought that can create
extreme values, then the Cauchy distribution may be a more reasonable
distribution. The same method also allow us to parameterise the Cauchy
distribution in which the uncertainty remains constant. The choice of
distribution should reflect our belief in the probability allocation
but it should not alter the uncertainty we have imposed initially.

Following the same procedure, we obtain the following parameters.


\begin{align}
  x_0 &= \Sexpr{obsValue}\nonumber\\ 
  \gamma &= e^{I - ln(4\pi)} = e^{-ln(\Sexpr{confidence}) - ln(4\pi)} \approx \Sexpr{round(parameterise(obsValue = obsValue, selfInformation = selfInfo, distribution = "cauchy")$scale, 4)}\nonumber
\end{align}

then,

\begin{align}
  W \sim Cauchy(\Sexpr{obsValue}, \Sexpr{round(parameterise(obsValue = obsValue, selfInformation = selfInfo, distribution = "cauchy")$scale, 4)})\nonumber
\end{align}



<<parameterise-cauchy>>=
parameterise(obsValue = obsValue, 
             selfInformation = selfInfo, 
             distribution = "cauchy")
@ 

Note, since the standard deviation of the Cauchy distribution is
undefined and thus it is impossible to parameterise the distribution
if we based our uncertainty measure on the size of the standard
deviation.


Moreover, we know production can not be negative and thus
distributions such as the Normal or the Cauchy distribution with
unbounded support may not be the appropriate distribution. A truncated
Normal distribution may incorporate this information by truncating
support and allow the variable to be defined strictly on the positive
real line.

Following the principle, we will arrive at a distribution which
preserves the uncertainty yet re-assign the probability to reflect the
physical condition that production can not be negative. In the case of
the truncated Normal distribution, analytical solution does not exist,
but a numerical solution is provided by the package.


<<parameterise-trnormal>>=
parameterise(obsValue = obsValue, 
             selfInformation = selfInfo, 
             distribution = "truncNorm")
@ 

and the resulting distribution is,


\begin{align}
  W \sim trN(\Sexpr{obsValue}, \Sexpr{round(parameterise(obsValue = obsValue, 
             selfInformation = selfInfo, 
             distribution = "truncNorm")$sd, 4)})\nonumber
\end{align}

When the mean is close to zero, the standard deviation of the
truncated normal is marginally larger than the normal distribution
above. This is due to the fact to maintain the same level of
uncertainty while reducing the support space, one has to increase the
standard deviation. However, as the mean increases, the truncated
normal becomes more like the normal distribution with very similar
standard deviation.

Finally, the log-Normal distribution is also another distribution
which is defined only on the real line that is suitable to describe
the probabiblity allocation of the wheat production.


<<parameterise-lognormal>>=
parameterise(obsValue = obsValue, 
             selfInformation = selfInfo, 
             distribution = "logNorm")
@ 

and,


\begin{align}
  W \sim lnN(\Sexpr{round(parameterise(obsValue = obsValue, 
    selfInformation = selfInfo, 
    distribution = "logNorm")$meanlog, 4)}, 
  \Sexpr{round(parameterise(obsValue = obsValue, 
    selfInformation = selfInfo, 
    distribution = "logNorm")$sdlog, 4)})\nonumber
\end{align}


Presented below is a comparison of the constructed distributions based
on the same observed value and uncertainty. We can see all the
distribution has mode at the observed value \Sexpr{obsValue}, except
for the exponential distribution which has mode at 0. The choice of
the distribution reflects our understanding of the element, but the
level of uncertainty is held the same following the framework.

<<plot, echo=FALSE, fig.height=5>>=
library(ggplot2)
cauchy = distributionise(obsValue = obsValue, selfInformation = selfInfo, distribution = "cauchy")
ggplot(data.frame(x = seq(cauchy$parameters$location - 10 * cauchy$parameters$scale, 
                      cauchy$parameters$location + 10 * cauchy$parameters$scale, length.out = 3)),
       aes(x)) +
    stat_function(fun = function(x) cauchy$pdf(x), geom = "line", aes(colour = "Cauchy")) +
    stat_function(fun = function(x){ 
                      distributionise(obsValue = obsValue, selfInformation = selfInfo, 
                                      distribution = "normal")$pdf(x)
                    }, geom = "line", aes(colour = "Normal")) +
    stat_function(fun = function(x){ 
                      distributionise(obsValue = obsValue, selfInformation = selfInfo, 
                                      distribution = "laplace")$pdf(x)
                  }, geom = "line", aes(colour = "Laplace")) +        
    stat_function(fun = function(x){
                      distributionise(obsValue = obsValue, selfInformation = selfInfo, 
                                      distribution = "truncNorm")$pdf(x)
                  }, geom = "line", aes(colour = "Truncated-Normal")) +
    stat_function(fun = function(x){
                      distributionise(obsValue = obsValue, selfInformation = selfInfo, 
                                      distribution = "exponential")$pdf(x)
                  }, geom = "line", aes(colour = "Exponential")) +
    stat_function(fun = function(x){
                      distributionise(obsValue = obsValue, selfInformation = selfInfo, 
                                      distribution = "logNorm")$pdf(x)
                  }, geom = "line", aes(colour = "Log-Normal")) + 
    stat_function(fun = function(x){
                      distributionise(obsValue = obsValue, selfInformation = selfInfo, 
                                      distribution = "weibull")$pdf(x)
                  }, geom = "line", aes(colour = "Weibull")) + 
    scale_colour_manual("", 
                        ## values = c("blue", "purple", "orange", "gold", "steelblue", "red"), 
                        values = brewer.pal(7, "Dark2"),
                        breaks = c("Cauchy", "Normal", "Laplace", "Truncated-Normal", "Exponential", "Log-Normal", "Weibull")) +  
    theme(legend.position="top") +
    labs(x = "", y = "")

@ 

The log-Normal and the truncated-Normal distribution both converges to
the Normal distribution when the mean is relatively large with respect
to the standard deviation.
  

%% Add a plot of all the distributions offered in the packag except
%% for the exponential distribution.

\section{Illustration}

To close the paper, we present a full case study of constructing the
distributions and utilise the distributions to perform the
probabilistic balancing.

Take the example data, the first step is to compute the level of
uncertainty of each FBS element based on the flags in the FBS.



<<calculate-fbs-weights>>=

## Select all the flag columns
flagColumns = grep("Flag", colnames(vignetteFBSTable), value = TRUE)
valueColumn = grep("Value", colnames(vignetteFBSTable), value = TRUE)

## First we convert the flags to weights or confidence
(weightsFBS.df = 
     data.frame(lapply(vignetteFBSTable[, flagColumns], 
                       function(x) {
                           flag2weight(flagObservationStatus = x, 
                                       flagTable = vignetteFlagTable)
                                    })))

## Then we compute the self-information
(selfInfoFBS.df = data.frame(lapply(weightsFBS.df, selfInformation)))



@ 


To create the distribution, we simply provide the function
\code{distributionise} the \textbf{observered value}, the \textbf{total
  information} computed from the flag and the \textbf{desired
  distribution}.

The function returns a list of two object. The first is the
distribution function with the parameters computed, while the second
object is a list with the corresponding values of the parameter.

<<create-production-distribution>>=

## Parameterise the production element with a Normal distribution
distributionise(obsValue = vignetteFBSTable$productionValue, 
                selfInformation = selfInfoFBS.df$productionFlag, 
                distribution = "normal")


## Parameterise the production element with a Truncated Normal distribution
distributionise(obsValue = vignetteFBSTable$productionValue, 
                selfInformation = selfInfoFBS.df$productionFlag, 
                distribution = "truncNorm")

@ 



Below we show a full process of how to construct each uncertainty
distribution and specify the constraints for the balancing of the FBS.

<<balance, eval=TRUE>>=
## Here we simplify the example with one distribution, 
## in practice each element can have their own corresponding distribution.
chosenDistribution = "truncNorm"

productionDist = 
    distributionise(obsValue = vignetteFBSTable$productionValue,
                    selfInformation = selfInfoFBS.df$productionFlag,
                    distribution = chosenDistribution)

importDist = 
    distributionise(obsValue = vignetteFBSTable$importValue,
                    selfInformation = selfInfoFBS.df$importFlag,
                    distribution = chosenDistribution)

exportDist = 
    distributionise(obsValue = vignetteFBSTable$exportValue,
                    selfInformation = selfInfoFBS.df$exportFlag,
                    distribution = chosenDistribution)

foodDist = 
    distributionise(obsValue = vignetteFBSTable$foodValue,
                    selfInformation = selfInfoFBS.df$foodFlag,
                    distribution = chosenDistribution)
lossDist = 
    distributionise(obsValue = vignetteFBSTable$lossValue,
                    selfInformation = selfInfoFBS.df$lossFlag,
                    distribution = chosenDistribution)


## Create the likelihood function from the distributions
ll = function(x){
    -log(productionDist$pdf(x[1])) - 
        log(importDist$pdf(x[2])) -
        log(exportDist$pdf(x[3])) - 
        log(foodDist$pdf(x[4])) - 
        log(lossDist$pdf(x[5]))
}

## Create the contraint funciton
constraint = function(x){
    ## (1) Productin + Import - Export - Food - Loss = 0
    ## (2) and (3) holding import and export constant
    c(x[1] + x[2] - x[3] - x[4] - x[5], x[2], x[3])
}

## Balance the Food Balance Sheet
library(Rsolnp)
## NOTE (Michael): If degenerate distribution is present, then take 
##                 a small step (small delta) and have large inner 
##                 iteration to ensure convergence to the right solution.
balancedFBS = 
    solnp(pars = as.numeric(vignetteFBSTable[, valueColumn]),
          fun = ll,
          eqfun = constraint,
          eqB = c(0, vignetteFBSTable$importValue, vignetteFBSTable$exportValue),
          LB = rep(0, length(valueColumn)),
          control = list(delta = 1e-1, trace = 2, inner.iter = 3000))

## Original values
as.numeric(vignetteFBSTable[, valueColumn])

## Balanced values
round(balancedFBS$par)
    

@ 

From the balanced value, we can see since the element Food has the
lowest confidence, thus its value is adjusted the most followed by
Production and Loss with Import and Export held constant.

In addition, since we have the same confidence in both the production
and the losses element while at the same time specified symmetric
distributions; the pair were adjusted towards the balancing by the
same magnitude.

%% \section{Discussion}

\end{document}
